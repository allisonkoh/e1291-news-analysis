{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing \n",
    "Normalization and other things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/allisonwun-\n",
      "[nltk_data]     huikoh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install pyspellchecker\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-processing before removing contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "train_df = pd.read_csv('/Users/allisonwun-huikoh/Google Drive/___hertie/2019-fall/e1291-ml/e1291-group-project/news-data/us_news_test.csv')\n",
    "train_df = train_df[train_df['headline'].notnull()]\n",
    "train_df = train_df[train_df['description'].notnull()]\n",
    "train_df = train_df[train_df['text'].notnull()]\n",
    "valid_df = pd.read_csv('/Users/allisonwun-huikoh/Google Drive/___hertie/2019-fall/e1291-ml/e1291-group-project/news-data/us_news_valid.csv')\n",
    "valid_df.columns = [col.replace('validate.', '') for col in valid_df.columns]\n",
    "valid_df = valid_df[valid_df['headline'].notnull()]\n",
    "valid_df = valid_df[valid_df['description'].notnull()]\n",
    "valid_df = valid_df[valid_df['text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outlet           object\n",
       "outlet_url       object\n",
       "datetime         object\n",
       "url_orig         object\n",
       "headline         object\n",
       "description      object\n",
       "author           object\n",
       "domain           object\n",
       "topic_tags       object\n",
       "text             object\n",
       "section          object\n",
       "news_keywords    object\n",
       "subsection       object\n",
       "paywall          object\n",
       "provider         object\n",
       "ideology         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting data \n",
    "valid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    " # lowercase \n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation \n",
    "train_df['headline'] = train_df['headline'].str.replace('[^\\w\\s]','')\n",
    "valid_df['headline'] = valid_df['headline'].str.replace('[^\\w\\s]','')\n",
    "train_df['description'] = train_df['description'].str.replace('[^\\w\\s]','')\n",
    "valid_df['description'] = valid_df['description'].str.replace('[^\\w\\s]','')\n",
    "train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\n",
    "valid_df['text'] = valid_df['text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords \n",
    "stop = stopwords.words('english')\n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common word removal \n",
    "freq1 = pd.Series(' '.join(train_df['headline']).split()).value_counts()[:10]\n",
    "freq1 = list(freq1.index)\n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq1))\n",
    "\n",
    "freq2 = pd.Series(' '.join(valid_df['headline']).split()).value_counts()[:10]\n",
    "freq2 = list(freq2.index)\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq2))\n",
    "\n",
    "freq3 = pd.Series(' '.join(train_df['description']).split()).value_counts()[:10]\n",
    "freq3 = list(freq3.index)\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq3))\n",
    "\n",
    "freq4 = pd.Series(' '.join(valid_df['description']).split()).value_counts()[:10]\n",
    "freq4 = list(freq4.index)\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq4))\n",
    "\n",
    "freq5 = pd.Series(' '.join(train_df['text']).split()).value_counts()[:10]\n",
    "freq5 = list(freq5.index)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq5))\n",
    "\n",
    "freq6 = pd.Series(' '.join(valid_df['text']).split()).value_counts()[:10]\n",
    "freq6 = list(freq6.index)\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### rare word removal\n",
    "freq_1 = pd.Series(' '.join(train_df['headline']).split()).value_counts()[-10:]\n",
    "freq_1 = list(freq_1.index)\n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_1))\n",
    "\n",
    "freq_2 = pd.Series(' '.join(valid_df['headline']).split()).value_counts()[-10:]\n",
    "freq_2 = list(freq_2.index)\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_2))\n",
    "\n",
    "freq_3 = pd.Series(' '.join(train_df['description']).split()).value_counts()[-10:]\n",
    "freq_3 = list(freq_3.index)\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_3))\n",
    "\n",
    "freq_4 = pd.Series(' '.join(valid_df['description']).split()).value_counts()[-10:]\n",
    "freq_4 = list(freq_4.index)\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_4))\n",
    "\n",
    "freq_5 = pd.Series(' '.join(train_df['text']).split()).value_counts()[-10:]\n",
    "freq_5 = list(freq_5.index)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_5))\n",
    "\n",
    "freq_6 = pd.Series(' '.join(valid_df['text']).split()).value_counts()[-10:]\n",
    "freq_6 = list(freq_6.index)\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling correction \n",
    "train_df['headline'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "valid_df['headline'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "train_df['description'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "valid_df['description'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "train_df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "valid_df['text'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer =  TweetTokenizer()\n",
    "# spell = SpellChecker()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_contractions(tokens):\n",
    "\ttoken_map = {\n",
    "\t\t\"ain't\" : 'is not',\n",
    "\t\t\"aint\" : 'is not',\n",
    "\t\t\"aren't\" : 'are not',\n",
    "\t\t\"can't\" : 'can not',\n",
    "\t\t\"cannot\" : 'can not',\n",
    "\t\t\"could've\" : 'could have',\n",
    "\t\t\"couldn't\" : 'could not',\n",
    "\t\t\"couldnt\" : 'could not',\n",
    "\t\t\"didn't\" : 'did not',\n",
    "\t\t\"didnt\" : 'did not',\n",
    "\t\t\"doesn't\" : 'does not',\n",
    "\t\t\"doesnt\" : 'does not',\n",
    "\t\t\"don't\" : 'do not',\n",
    "\t\t\"gonna'\" : 'going to',\n",
    "\t\t\"gotta'\" : 'got to',\n",
    "\t\t\"hadn't\" : 'had not',\n",
    "\t\t\"hasn't\" : 'has not',\n",
    "\t\t\"haven't\" : 'have not',\n",
    "\t\t\"he'll\" : 'he will',\n",
    "\t\t\"he's\" : 'he is',\n",
    "\t\t\"he've\" : 'he have',\n",
    "\t\t\"how'd\" : 'how did',\n",
    "\t\t\"how'll\" : 'how will',\n",
    "\t\t\"how're\" : 'how are',\n",
    "\t\t\"how's\" : 'how is',\n",
    "\t\t\"i'd\" : 'i would',\n",
    "\t\t\"i'll\" : 'i will',\n",
    "\t\t\"i'm\" : 'i am',\n",
    "\t\t\"i'mm\" : 'i am',\n",
    "\t\t\"i've\" : 'i have',\n",
    "\t\t\"iäm\" : 'i am',\n",
    "\t\t\"isn't\" : 'is not',\n",
    "\t\t\"isnt\" : 'is not',\n",
    "\t\t\"it'd\" : 'it would',\n",
    "\t\t\"it'll\" : 'it shall',\n",
    "\t\t\"it's\" : 'it is',\n",
    "\t\t\"let's\" : 'let us',\n",
    "\t\t\"she'll\" : 'she will',\n",
    "\t\t\"she's\" : 'she is',\n",
    "\t\t\"should've\" : 'should have',\n",
    "\t\t\"shouldn't\" : 'should not',\n",
    "\t\t\"shouldnt\" : 'should not',\n",
    "\t\t\"that'll\" : 'that will',\n",
    "\t\t\"that's\" : 'that is',\n",
    "\t\t\"there's\" : 'there is',\n",
    "\t\t\"they'll\" : 'they will',\n",
    "\t\t\"they're\" : 'they are',\n",
    "\t\t\"theyre\" : 'they are',\n",
    "\t\t\"theyve\" : 'they have',\n",
    "\t\t\"wasn't\" : 'was not',\n",
    "\t\t\"we'll\" : 'we will',\n",
    "\t\t\"we'r\" : 'we are',\n",
    "\t\t\"we're\" : 'we are',\n",
    "\t\t\"we've\" : 'we have',\n",
    "\t\t\"weren't\" : 'were not',\n",
    "\t\t\"what's\" : 'what is',\n",
    "\t\t\"who'll\" : 'who will',\n",
    "\t\t\"who're\" : 'who are',\n",
    "\t\t\"who's\" : 'who is',\n",
    "\t\t\"why're\" : 'why are',\n",
    "\t\t\"won't\" : 'will not',\n",
    "\t\t\"would've\" : 'would have',\n",
    "\t\t\"wouldn't\" : 'would not',\n",
    "\t\t\"wouldnt\" : 'would not',\n",
    "\t\t\"you'll\" : 'you will',\n",
    "\t\t\"you're\" : 'you are',\n",
    "\t\t\"you've\" : 'you have'\n",
    "\t}\n",
    "\tnorm_tokens = []\n",
    "\tfor t in tokens:\n",
    "\t\tif t in token_map.keys():\n",
    "\t\t\tfor item in token_map[t].split():\n",
    "\t\t\t\tnorm_tokens.append(item)\n",
    "\t\telse:\n",
    "\t\t\tfor item in re.split('\\W+', t.replace(\"'s\",\"\")):\n",
    "\t\t\t\tif item != '':\n",
    "\t\t\t\t\tnorm_tokens.append(item)\n",
    "\treturn(norm_tokens)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\tif treebank_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif treebank_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif treebank_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif treebank_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn None\n",
    "    \n",
    "def lemma_tokenizer(text):\n",
    "\ttext = text.replace('’',\"'\")\n",
    "\ttokens = normalize_contractions(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outlet           object\n",
       "outlet_url       object\n",
       "datetime         object\n",
       "url_orig         object\n",
       "headline         object\n",
       "description      object\n",
       "author           object\n",
       "domain           object\n",
       "topic_tags       object\n",
       "text             object\n",
       "section          object\n",
       "news_keywords    object\n",
       "subsection       object\n",
       "paywall          object\n",
       "provider         object\n",
       "ideology         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['headline'] = train_df['headline'].apply(lemma_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlet</th>\n",
       "      <th>outlet_url</th>\n",
       "      <th>datetime</th>\n",
       "      <th>url_orig</th>\n",
       "      <th>headline</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic_tags</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>news_keywords</th>\n",
       "      <th>subsection</th>\n",
       "      <th>paywall</th>\n",
       "      <th>provider</th>\n",
       "      <th>ideology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>thehill</td>\n",
       "      <td>https://www.thehill.com/</td>\n",
       "      <td>2018-05-03T19:09:42Z</td>\n",
       "      <td>http://thehill.com/homenews/administration/386...</td>\n",
       "      <td>None</td>\n",
       "      <td>“i am waiting for the attorney general to step...</td>\n",
       "      <td>Niall Stanage</td>\n",
       "      <td>Administration</td>\n",
       "      <td>Russia Investigation, Rudy Giuliani, Michael C...</td>\n",
       "      <td>rudy giuliani called for attorney general jeff...</td>\n",
       "      <td>Homenews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/</td>\n",
       "      <td>2019-01-31T09:12:05Z</td>\n",
       "      <td>https://www.cnbc.com/2019/01/31/us-bonds-feder...</td>\n",
       "      <td>None</td>\n",
       "      <td>u.s. government debt prices were higher thursd...</td>\n",
       "      <td>Sam Meredith</td>\n",
       "      <td>Bonds</td>\n",
       "      <td>Bonds, Central banking, World Markets, Bitcoin...</td>\n",
       "      <td>u.s. government debt prices were higher thursd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US treasury notes, bonds, economic data, gover...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>thehill</td>\n",
       "      <td>https://www.thehill.com/</td>\n",
       "      <td>2019-03-23T00:17:15Z</td>\n",
       "      <td>https://thehill.com/homenews/news/435392-sam-a...</td>\n",
       "      <td>None</td>\n",
       "      <td>samuel adams is releasing a new beer inspired ...</td>\n",
       "      <td>Brooke Seipel</td>\n",
       "      <td>News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>samuel adams is releasing a new beer inspired ...</td>\n",
       "      <td>Homenews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bbcnews</td>\n",
       "      <td>https://www.bbc.co.uk/news</td>\n",
       "      <td>2019-04-17T17:00:27Z</td>\n",
       "      <td>https://www.bbc.co.uk/news/health-47960874</td>\n",
       "      <td>None</td>\n",
       "      <td>the study could aid medical research and fuel ...</td>\n",
       "      <td>James Gallagher</td>\n",
       "      <td>Health</td>\n",
       "      <td>Alzheimer's, Pigs, United States, Medicine, Me...</td>\n",
       "      <td>us scientists have partially revived pig brain...</td>\n",
       "      <td>News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>https://www.cnbc.com/</td>\n",
       "      <td>2019-02-25T08:33:42Z</td>\n",
       "      <td>https://www.cnbc.com/2019/02/25/stock-market-i...</td>\n",
       "      <td>None</td>\n",
       "      <td>u.s. stock index futures pointed to a higher s...</td>\n",
       "      <td>Spriha Srivastava</td>\n",
       "      <td>U.S. Markets</td>\n",
       "      <td>Xi Jinping, Breaking News: Markets, Pre-market...</td>\n",
       "      <td>u.s. stock index futures pointed to a higher s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US stock futures, Wall Street pre-markets, US ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>https://www.breitbart.com/</td>\n",
       "      <td>2018-08-16T19:29:14Z</td>\n",
       "      <td>https://www.breitbart.com/big-government/2018/...</td>\n",
       "      <td>None</td>\n",
       "      <td>president donald trump opened a cabinet meetin...</td>\n",
       "      <td>Charlie Spiering</td>\n",
       "      <td>Big Government</td>\n",
       "      <td>Cabinet Meeting, Donald Trump, Freedom Of The ...</td>\n",
       "      <td>“if you’d like, you can stay. if you’d like, y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>infowars</td>\n",
       "      <td>https://www.infowars.com/</td>\n",
       "      <td>2018-05-04T09:39:42Z</td>\n",
       "      <td>https://www.infowars.com/mueller-goes-after-ro...</td>\n",
       "      <td>None</td>\n",
       "      <td>focus on alleged interactions between former t...</td>\n",
       "      <td>Brian Schwartz</td>\n",
       "      <td>Featured Stories</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special counsel robert mueller is focusing int...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>https://www.breitbart.com/</td>\n",
       "      <td>2019-04-11T21:26:12Z</td>\n",
       "      <td>https://www.breitbart.com/national-security/20...</td>\n",
       "      <td>None</td>\n",
       "      <td>julian assange's arrest left many questioning ...</td>\n",
       "      <td>Frances Martel</td>\n",
       "      <td>National Security</td>\n",
       "      <td>Bradley Manning, Ecuador, Julian Assange, Lati...</td>\n",
       "      <td>the move by president lenin moreno – a sociali...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>https://www.breitbart.com/</td>\n",
       "      <td>2018-11-17T01:36:21Z</td>\n",
       "      <td>https://www.breitbart.com/tech/2018/11/16/flas...</td>\n",
       "      <td>None</td>\n",
       "      <td>hillary clinton's office ignored julian assang...</td>\n",
       "      <td>Allum Bokhari</td>\n",
       "      <td>Tech</td>\n",
       "      <td>Clinton Email Scandal, Hillary Clinton, Hillar...</td>\n",
       "      <td>the department of justice has obtained sealed ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>https://www.breitbart.com/</td>\n",
       "      <td>2019-01-28T15:26:06Z</td>\n",
       "      <td>https://www.breitbart.com/entertainment/2019/0...</td>\n",
       "      <td>None</td>\n",
       "      <td>matthew mcconaughey notched one of the worst d...</td>\n",
       "      <td>Breitbart News</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Bohemian Rhapsody, Box Office, Glass, Green Bo...</td>\n",
       "      <td>the weekend’s two new wide releases — mcconaug...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9824 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         outlet                  outlet_url              datetime  \\\n",
       "0       thehill    https://www.thehill.com/  2018-05-03T19:09:42Z   \n",
       "1          cnbc       https://www.cnbc.com/  2019-01-31T09:12:05Z   \n",
       "2       thehill    https://www.thehill.com/  2019-03-23T00:17:15Z   \n",
       "3       bbcnews  https://www.bbc.co.uk/news  2019-04-17T17:00:27Z   \n",
       "4          cnbc       https://www.cnbc.com/  2019-02-25T08:33:42Z   \n",
       "...         ...                         ...                   ...   \n",
       "9995  breitbart  https://www.breitbart.com/  2018-08-16T19:29:14Z   \n",
       "9996   infowars   https://www.infowars.com/  2018-05-04T09:39:42Z   \n",
       "9997  breitbart  https://www.breitbart.com/  2019-04-11T21:26:12Z   \n",
       "9998  breitbart  https://www.breitbart.com/  2018-11-17T01:36:21Z   \n",
       "9999  breitbart  https://www.breitbart.com/  2019-01-28T15:26:06Z   \n",
       "\n",
       "                                               url_orig headline  \\\n",
       "0     http://thehill.com/homenews/administration/386...     None   \n",
       "1     https://www.cnbc.com/2019/01/31/us-bonds-feder...     None   \n",
       "2     https://thehill.com/homenews/news/435392-sam-a...     None   \n",
       "3            https://www.bbc.co.uk/news/health-47960874     None   \n",
       "4     https://www.cnbc.com/2019/02/25/stock-market-i...     None   \n",
       "...                                                 ...      ...   \n",
       "9995  https://www.breitbart.com/big-government/2018/...     None   \n",
       "9996  https://www.infowars.com/mueller-goes-after-ro...     None   \n",
       "9997  https://www.breitbart.com/national-security/20...     None   \n",
       "9998  https://www.breitbart.com/tech/2018/11/16/flas...     None   \n",
       "9999  https://www.breitbart.com/entertainment/2019/0...     None   \n",
       "\n",
       "                                            description             author  \\\n",
       "0     “i am waiting for the attorney general to step...      Niall Stanage   \n",
       "1     u.s. government debt prices were higher thursd...       Sam Meredith   \n",
       "2     samuel adams is releasing a new beer inspired ...      Brooke Seipel   \n",
       "3     the study could aid medical research and fuel ...    James Gallagher   \n",
       "4     u.s. stock index futures pointed to a higher s...  Spriha Srivastava   \n",
       "...                                                 ...                ...   \n",
       "9995  president donald trump opened a cabinet meetin...   Charlie Spiering   \n",
       "9996  focus on alleged interactions between former t...     Brian Schwartz   \n",
       "9997  julian assange's arrest left many questioning ...     Frances Martel   \n",
       "9998  hillary clinton's office ignored julian assang...      Allum Bokhari   \n",
       "9999  matthew mcconaughey notched one of the worst d...     Breitbart News   \n",
       "\n",
       "                 domain                                         topic_tags  \\\n",
       "0        Administration  Russia Investigation, Rudy Giuliani, Michael C...   \n",
       "1                 Bonds  Bonds, Central banking, World Markets, Bitcoin...   \n",
       "2                  News                                                NaN   \n",
       "3                Health  Alzheimer's, Pigs, United States, Medicine, Me...   \n",
       "4          U.S. Markets  Xi Jinping, Breaking News: Markets, Pre-market...   \n",
       "...                 ...                                                ...   \n",
       "9995     Big Government  Cabinet Meeting, Donald Trump, Freedom Of The ...   \n",
       "9996   Featured Stories                                                NaN   \n",
       "9997  National Security  Bradley Manning, Ecuador, Julian Assange, Lati...   \n",
       "9998               Tech  Clinton Email Scandal, Hillary Clinton, Hillar...   \n",
       "9999      Entertainment  Bohemian Rhapsody, Box Office, Glass, Green Bo...   \n",
       "\n",
       "                                                   text   section  \\\n",
       "0     rudy giuliani called for attorney general jeff...  Homenews   \n",
       "1     u.s. government debt prices were higher thursd...       NaN   \n",
       "2     samuel adams is releasing a new beer inspired ...  Homenews   \n",
       "3     us scientists have partially revived pig brain...      News   \n",
       "4     u.s. stock index futures pointed to a higher s...       NaN   \n",
       "...                                                 ...       ...   \n",
       "9995  “if you’d like, you can stay. if you’d like, y...       NaN   \n",
       "9996  special counsel robert mueller is focusing int...       NaN   \n",
       "9997  the move by president lenin moreno – a sociali...       NaN   \n",
       "9998  the department of justice has obtained sealed ...       NaN   \n",
       "9999  the weekend’s two new wide releases — mcconaug...       NaN   \n",
       "\n",
       "                                          news_keywords subsection paywall  \\\n",
       "0                                                   NaN        NaN     NaN   \n",
       "1     US treasury notes, bonds, economic data, gover...        NaN     NaN   \n",
       "2                                                   NaN        NaN     NaN   \n",
       "3                                                   NaN        NaN     NaN   \n",
       "4     US stock futures, Wall Street pre-markets, US ...        NaN     NaN   \n",
       "...                                                 ...        ...     ...   \n",
       "9995                                                NaN        NaN     NaN   \n",
       "9996                                                NaN        NaN     NaN   \n",
       "9997                                                NaN        NaN     NaN   \n",
       "9998                                                NaN        NaN     NaN   \n",
       "9999                                                NaN        NaN     NaN   \n",
       "\n",
       "     provider ideology  \n",
       "0         NaN   center  \n",
       "1         NaN   center  \n",
       "2         NaN   center  \n",
       "3         NaN   center  \n",
       "4         NaN   center  \n",
       "...       ...      ...  \n",
       "9995      NaN    right  \n",
       "9996      NaN    right  \n",
       "9997      NaN    right  \n",
       "9998      NaN    right  \n",
       "9999      NaN    right  \n",
       "\n",
       "[9824 rows x 16 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
