{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing \n",
    "Normalization and other things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspellchecker\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre-processing before removing contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "train_df = pd.read_csv('/Users/allisonwun-huikoh/Google Drive/___hertie/2019-fall/e1291-ml/e1291-group-project/news-data/us_news_test.csv')\n",
    "train_df = train_df[train_df['headline'].notnull()]\n",
    "train_df = train_df[train_df['description'].notnull()]\n",
    "train_df = train_df[train_df['text'].notnull()]\n",
    "valid_df = pd.read_csv('/Users/allisonwun-huikoh/Google Drive/___hertie/2019-fall/e1291-ml/e1291-group-project/news-data/us_news_valid.csv')\n",
    "valid_df.columns = [col.replace('validate.', '') for col in valid_df.columns]\n",
    "valid_df = valid_df[valid_df['headline'].notnull()]\n",
    "valid_df = valid_df[valid_df['description'].notnull()]\n",
    "valid_df = valid_df[valid_df['text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outlet           object\n",
       "outlet_url       object\n",
       "datetime         object\n",
       "url_orig         object\n",
       "headline         object\n",
       "description      object\n",
       "author           object\n",
       "domain           object\n",
       "topic_tags       object\n",
       "text             object\n",
       "section          object\n",
       "news_keywords    object\n",
       "subsection       object\n",
       "paywall          object\n",
       "provider         object\n",
       "ideology         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting data \n",
    "valid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    " # lowercase \n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation \n",
    "train_df['headline'] = train_df['headline'].str.replace('[^\\w\\s]','')\n",
    "valid_df['headline'] = valid_df['headline'].str.replace('[^\\w\\s]','')\n",
    "train_df['description'] = train_df['description'].str.replace('[^\\w\\s]','')\n",
    "valid_df['description'] = valid_df['description'].str.replace('[^\\w\\s]','')\n",
    "train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\n",
    "valid_df['text'] = valid_df['text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords \n",
    "stop = stopwords.words('english')\n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common word removal \n",
    "freq1 = pd.Series(' '.join(train_df['headline']).split()).value_counts()[:10]\n",
    "freq1 = list(freq1.index)\n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq1))\n",
    "\n",
    "freq2 = pd.Series(' '.join(valid_df['headline']).split()).value_counts()[:10]\n",
    "freq2 = list(freq2.index)\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq2))\n",
    "\n",
    "freq3 = pd.Series(' '.join(train_df['description']).split()).value_counts()[:10]\n",
    "freq3 = list(freq3.index)\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq3))\n",
    "\n",
    "freq4 = pd.Series(' '.join(valid_df['description']).split()).value_counts()[:10]\n",
    "freq4 = list(freq4.index)\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq4))\n",
    "\n",
    "freq5 = pd.Series(' '.join(train_df['text']).split()).value_counts()[:10]\n",
    "freq5 = list(freq5.index)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq5))\n",
    "\n",
    "freq6 = pd.Series(' '.join(valid_df['text']).split()).value_counts()[:10]\n",
    "freq6 = list(freq6.index)\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### rare word removal\n",
    "freq_1 = pd.Series(' '.join(train_df['headline']).split()).value_counts()[-10:]\n",
    "freq_1 = list(freq_1.index)\n",
    "train_df['headline'] = train_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_1))\n",
    "\n",
    "freq_2 = pd.Series(' '.join(valid_df['headline']).split()).value_counts()[-10:]\n",
    "freq_2 = list(freq_2.index)\n",
    "valid_df['headline'] = valid_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_2))\n",
    "\n",
    "freq_3 = pd.Series(' '.join(train_df['description']).split()).value_counts()[-10:]\n",
    "freq_3 = list(freq_3.index)\n",
    "train_df['description'] = train_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_3))\n",
    "\n",
    "freq_4 = pd.Series(' '.join(valid_df['description']).split()).value_counts()[-10:]\n",
    "freq_4 = list(freq_4.index)\n",
    "valid_df['description'] = valid_df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_4))\n",
    "\n",
    "freq_5 = pd.Series(' '.join(train_df['text']).split()).value_counts()[-10:]\n",
    "freq_5 = list(freq_5.index)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_5))\n",
    "\n",
    "freq_6 = pd.Series(' '.join(valid_df['text']).split()).value_counts()[-10:]\n",
    "freq_6 = list(freq_6.index)\n",
    "valid_df['text'] = valid_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling correction \n",
    "train_df['headline'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "valid_df['headline'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "train_df['description'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "valid_df['description'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "train_df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "valid_df['text'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer =  TweetTokenizer()\n",
    "# spell = SpellChecker()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_contractions(tokens):\n",
    "\ttoken_map = {\n",
    "\t\t\"ain't\" : 'is not',\n",
    "\t\t\"aint\" : 'is not',\n",
    "\t\t\"aren't\" : 'are not',\n",
    "\t\t\"can't\" : 'can not',\n",
    "\t\t\"cannot\" : 'can not',\n",
    "\t\t\"could've\" : 'could have',\n",
    "\t\t\"couldn't\" : 'could not',\n",
    "\t\t\"couldnt\" : 'could not',\n",
    "\t\t\"didn't\" : 'did not',\n",
    "\t\t\"didnt\" : 'did not',\n",
    "\t\t\"doesn't\" : 'does not',\n",
    "\t\t\"doesnt\" : 'does not',\n",
    "\t\t\"don't\" : 'do not',\n",
    "\t\t\"gonna'\" : 'going to',\n",
    "\t\t\"gotta'\" : 'got to',\n",
    "\t\t\"hadn't\" : 'had not',\n",
    "\t\t\"hasn't\" : 'has not',\n",
    "\t\t\"haven't\" : 'have not',\n",
    "\t\t\"he'll\" : 'he will',\n",
    "\t\t\"he's\" : 'he is',\n",
    "\t\t\"he've\" : 'he have',\n",
    "\t\t\"how'd\" : 'how did',\n",
    "\t\t\"how'll\" : 'how will',\n",
    "\t\t\"how're\" : 'how are',\n",
    "\t\t\"how's\" : 'how is',\n",
    "\t\t\"i'd\" : 'i would',\n",
    "\t\t\"i'll\" : 'i will',\n",
    "\t\t\"i'm\" : 'i am',\n",
    "\t\t\"i'mm\" : 'i am',\n",
    "\t\t\"i've\" : 'i have',\n",
    "\t\t\"iäm\" : 'i am',\n",
    "\t\t\"isn't\" : 'is not',\n",
    "\t\t\"isnt\" : 'is not',\n",
    "\t\t\"it'd\" : 'it would',\n",
    "\t\t\"it'll\" : 'it shall',\n",
    "\t\t\"it's\" : 'it is',\n",
    "\t\t\"let's\" : 'let us',\n",
    "\t\t\"she'll\" : 'she will',\n",
    "\t\t\"she's\" : 'she is',\n",
    "\t\t\"should've\" : 'should have',\n",
    "\t\t\"shouldn't\" : 'should not',\n",
    "\t\t\"shouldnt\" : 'should not',\n",
    "\t\t\"that'll\" : 'that will',\n",
    "\t\t\"that's\" : 'that is',\n",
    "\t\t\"there's\" : 'there is',\n",
    "\t\t\"they'll\" : 'they will',\n",
    "\t\t\"they're\" : 'they are',\n",
    "\t\t\"theyre\" : 'they are',\n",
    "\t\t\"theyve\" : 'they have',\n",
    "\t\t\"wasn't\" : 'was not',\n",
    "\t\t\"we'll\" : 'we will',\n",
    "\t\t\"we'r\" : 'we are',\n",
    "\t\t\"we're\" : 'we are',\n",
    "\t\t\"we've\" : 'we have',\n",
    "\t\t\"weren't\" : 'were not',\n",
    "\t\t\"what's\" : 'what is',\n",
    "\t\t\"who'll\" : 'who will',\n",
    "\t\t\"who're\" : 'who are',\n",
    "\t\t\"who's\" : 'who is',\n",
    "\t\t\"why're\" : 'why are',\n",
    "\t\t\"won't\" : 'will not',\n",
    "\t\t\"would've\" : 'would have',\n",
    "\t\t\"wouldn't\" : 'would not',\n",
    "\t\t\"wouldnt\" : 'would not',\n",
    "\t\t\"you'll\" : 'you will',\n",
    "\t\t\"you're\" : 'you are',\n",
    "\t\t\"you've\" : 'you have'\n",
    "\t}\n",
    "\tnorm_tokens = []\n",
    "\tfor t in tokens:\n",
    "\t\tif t in token_map.keys():\n",
    "\t\t\tfor item in token_map[t].split():\n",
    "\t\t\t\tnorm_tokens.append(item)\n",
    "\t\telse:\n",
    "\t\t\tfor item in re.split('\\W+', t.replace(\"'s\",\"\")):\n",
    "\t\t\t\tif item != '':\n",
    "\t\t\t\t\tnorm_tokens.append(item)\n",
    "\treturn(norm_tokens)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\tif treebank_tag.startswith('J'):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif treebank_tag.startswith('V'):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif treebank_tag.startswith('N'):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif treebank_tag.startswith('R'):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn None\n",
    "    \n",
    "def lemma_tokenizer(text):\n",
    "\ttext = text.replace('’',\"'\")\n",
    "\ttokens = normalize_contractions(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outlet           object\n",
       "outlet_url       object\n",
       "datetime         object\n",
       "url_orig         object\n",
       "headline         object\n",
       "description      object\n",
       "author           object\n",
       "domain           object\n",
       "topic_tags       object\n",
       "text             object\n",
       "section          object\n",
       "news_keywords    object\n",
       "subsection       object\n",
       "paywall          object\n",
       "provider         object\n",
       "ideology         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['headline'] = train_df['headline'].apply(lemma_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
